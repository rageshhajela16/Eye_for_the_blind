{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import array\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport string\nimport os\nimport time\nfrom PIL import Image\nimport glob\nfrom pickle import dump, load\nfrom time import time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#images='../input/flickr-image-dataset/flickr30k_images/flickr30k_images'\n#text_file='../input/flickr-image-dataset/flickr30k_images/results.csv'\n\nimages='../input/flickr8k/Images'\ntext_file='../input/flickr8k/captions.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_imgs = glob.glob(images + '/*.jpg',recursive=True)\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_rows = 5\nnum_cols = 4\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(mpimg.imread(all_imgs[i]));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_doc(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = load_doc(text_file)\nprint(doc[:300])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#/ this is for flickr30k dataset operation/\nall_img_id=[]\nall_img_vector=[]\nannotations=[]\nf=open(text_file,\"r\")\nfor line in f.readlines()[1:]:\n    img_id=line.split('|',1)[0]\n    cap=line.split('|')[-1]\n    all_img_id.append(img_id)\n    all_img_vector.append(images+'/'+img_id)\n    annotations.append(cap.split('\\n')[0])\n\n    #print(img_id,cap)\nlimit = 100000\nall_img_id = all_img_id[:limit]\nall_img_vector = all_img_vector[:limit]\nannotations=annotations[:limit]\ndf = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n    \ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#/ this is for flickr8k dataset operation/\n\nall_img_id=[]\nall_img_vector=[]\nannotations=[]\nf=open(text_file,\"r\")\nfor line in f.readlines()[1:]:\n    img_id, cap = line.split(',', 1)\n    img_id=img_id.split('.')[0]\n\n    all_img_id.append(img_id)\n    all_img_vector.append(images+'/'+img_id+'.jpg')\n    annotations.append(cap.split('\\n')[0])\n    \ndf = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n    \ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path=list(set(df.Path[:15]))\ncount = 1\nj=0\nfor path in img_path:\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, [224, 224])\n      \n    fig = plt.figure(figsize=(10,20))\n    ax = fig.add_subplot(5,2,count,xticks=[],yticks=[])\n    ax.imshow(img)\n    \n\n    count += 1\n      \n    ax = fig.add_subplot(5,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,5)\n    \n    img_cap=df.loc[df['Path'] == path, 'Captions']\n    \n    for i in range(len(img_cap)):\n        ax.text(0,i,img_cap.iloc[i],fontsize=15)\n    count += 1\n    j+=1\nplt.show()\n\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotations=[]\nfor cap in df.Captions:\n    cap='<start> '+ cap + ' <end>'\n    annotations.append(cap)\nannotations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_img_vector=[]\nfor img_path in df.Path:\n    all_img_vector.append(img_path)\n\nall_img_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_vocabulary():\n    vocab = []\n    for cap in df.Captions.values:\n        vocab.extend(cap.split())\n    print(\"Size of vocabulary: {}\".format(len(set(vocab))))\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary=gen_vocabulary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_count=Counter(vocabulary)\nval_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sort_lst = val_count.most_common(30)\nstop=set(stopwords.words('english'))\nx, y=[], []\nfor word,count in sort_lst:\n    if (word.lower() not in stop):\n        x.append(word)\n        y.append(count)\n            \nsns.barplot(x=y,y=x).set_title('Top 30 occurinng words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('../output/kaggle/working/files')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\ndef save_obj(obj, name ):\n    with open('../working/'+ name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\ndef load_obj(name):\n    with open('../working/' + name + '.pkl', 'rb') as f:\n        return pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_obj(df, \"dataframe\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=load_obj('dataframe')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_model = keras.applications.InceptionV3(include_top=False,weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nbase_model = keras.Model(new_input, hidden_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = preprocess_input(img)\n    return img, image_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nencode_train = sorted(set(all_img_vector))\n\nfeature_dict = {}\n\n\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(encode_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfor img, path in tqdm(image_dataset):\n  batch_features = base_model(img)\n  batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    feature_dict[path_of_feature] =  bf.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()\nlen(feature_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_k = 5000\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=top_k,oov_token=\"<unk>\",filters='!\"#$%&()*+.-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(annotations)\n\ntrain_seqs = tokenizer.texts_to_sequences(annotations)\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\ntrain_seqs = tokenizer.texts_to_sequences(annotations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotations[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seqs[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\nx, y=[], []\nfor word,count in sorted_by_word_count[:30]:\n    if (word.lower() not in stop):\n        x.append(word)\n        y.append(count)\n            \nsns.barplot(x=y,y=x).set_title('Top 30 occurinng words')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_max_length(tensor):\n    max_t=max(len(t) for t in tensor)\n    print(\"The maximum length of a sentence in the annotation dataset is: \" + str(max_t) )\n    return max_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = calc_max_length(train_seqs)\ncap_vector = keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen=max_length)\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_train, img_test, cap_train, cap_test = train_test_split(all_img_vector,cap_vector,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training data for images: \" + str(len(img_train)))\nprint(\"Testing data for images: \" + str(len(img_test)))\nprint(\"Training data for Captions: \" + str(len(cap_train)))\nprint(\"Testing data for Captions: \" + str(len(cap_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_obj(img_train, \"img_train\")\nsave_obj(cap_train, \"cap_train\")\nsave_obj(img_test, \"img_test\")\nsave_obj(cap_test, \"cap_test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_func(img_name, cap):\n  img_tensor = feature_dict[img_name.decode('utf-8')]\n  return img_tensor, cap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nvocab_size = top_k + 1\ntrain_num_steps = len(img_train) // BATCH_SIZE\ntest_num_steps = len(img_test) // BATCH_SIZE\nfeatures_shape = bf.shape[1]\nattention_features_shape = bf.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_dataset(img_data, cap_data):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((img_data, cap_data))\n    dataset = dataset.shuffle(BUFFER_SIZE)\n\n    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n\n\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset=gen_dataset(img_train,cap_train)\ntest_dataset=gen_dataset(img_test,cap_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = train_dataset.take(1)\nfor img,cap in dataset:\n  print(img.shape)\n  print(cap.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN_Encoder(keras.Model):\n\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        self.fc = layers.Dense(embedding_dim)\n        self.dropout = layers.Dropout(0.5)\n        \n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x\n\nencoder = CNN_Encoder(embedding_dim)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention_model(keras.Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = layers.Dense(units)\n        self.W2 = layers.Dense(units)\n        self.V = layers.Dense(1)\n\n    def call(self,features,hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # score shape == (batch_size, 64, hidden_size)\n        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n\n    # attention_weights shape == (batch_size, 64, 1)\n    # you get 1 at the last axis because you are applying score to self.V\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN_Decoder(Model):\n    def __init__(self, embedding_dim, units, vocab_size):\n\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n\n        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n        self.gru = layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n        \n        self.fc1 = layers.Dense(self.units)\n        self.fc2 = layers.Dense(vocab_size)\n\n        self.attention = Attention_model(self.units)\n\n    def call(self, x, features, hidden):\n    # defining attention as a separate model\n        context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n        x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n        x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n        x = self.fc2(x)\n\n        return x, state, attention_weights\n\n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = layers.Dense(units)\n        self.W2 = layers.Dense(units)\n        self.V = layers.Dense(1)\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 64, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(hidden, 1) # shape: (batch_size, 1, hidden_size)\n        score = keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) #shape: (batch_size, 64, units)\n        attention_weights = keras.activations.softmax(self.V(score), axis=1) #shape: (batch_size, 64, 1)\n        context_vector = attention_weights * features #shape: (batch_size, 64,embedding_dim)\n        context_vector = tf.reduce_sum(context_vector, axis=1)#shape: (batch_size, embedding_dim)\n        \n\n        return context_vector, attention_weights\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units)\n        self.embed = layers.Embedding(vocab_size, embed_dim,mask_zero=True)\n        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n        self.d1 = layers.Dense(self.units)\n        self.d2 = layers.Dense(vocab_size)\n        self.dropout = Dropout(0.5)\n        \n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden)\n        embed = self.dropout(self.embed(x)) #shape: (batch_size, 1, embedding_dim)\n        mask = self.embed.compute_mask(x)\n        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) #shape: (batch_size, 1, embedding_dim + embedding_dim)\n        output,state = self.gru(embed,mask=mask) # output shape : (batch_size, max_length, hidden_size)\n        output = self.d1(output)\n        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n        \n        return output,state, attention_weights\n    \n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))\n    \ndecoder=Decoder(embedding_dim, units, vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention=Attention_model(units)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = RNN_Decoder(embedding_dim, units, vocab_size)\nattention=Attention_model(units)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape) \nprint(sample_cap_batch.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=encoder(sample_img_batch)\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=encoder(sample_img_batch)\n\nhidden = decoder.reset_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\nprint(hidden.shape)\nprint(dec_input.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_dataset = train_dataset.take(1)\nfor img,cap in dummy_dataset:\n    features=encoder(img)\n    hidden = decoder.reset_state(batch_size=cap.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * cap.shape[0], 1)\n    context_vector, attention_weights=attention(features, hidden)\n    predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n    print(features.shape)\n    print(predictions.shape)\n    print(attention_weights.shape)\n    print(context_vector.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\noptimizer = keras.optimizers.Adam()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('../output/kaggle/working/checkpoints/train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path = \"..working/checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n          loss += loss_function(target[:, i], predictions)\n          dec_input = tf.expand_dims(target[:, i], 1)\n        \n  avg_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, trainable_variables)\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, avg_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n          loss += loss_function(target[:, i], predictions)\n          dec_input = tf.expand_dims(target[:, i], 1)\n        \n  avg_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, trainable_variables)\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, avg_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_loss_cal(test_dataset):\n    \n  total_loss = 0\n  for (batch, (img_tensor, target)) in enumerate(test_dataset):\n    batch_loss, t_loss = test_step(img_tensor, target)\n    \n    total_loss += t_loss\n    avg_test_loss=total_loss/test_num_steps\n  return avg_test_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_plot = []\ntest_loss_plot = []\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss / train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n      print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n      best_test_loss = test_loss\n      ckpt_manager.save()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"..working/checkpoints/train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = base_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def beam_evaluate(image, beam_index = 3):\n    start = [tokenizer.word_index['<start>']]\n    result = [[start, 0.0]]\n\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = base_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n\n    while len(result[0][0]) < max_length:\n        i=0\n        temp = []\n        for s in result:\n            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n            i=i+1\n            word_preds = np.argsort(predictions[0])[-beam_index:]\n          \n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n            \n                prob += np.log(predictions[0][w])\n                    \n                temp.append([next_cap, prob])\n        result = temp\n        result = sorted(result, reverse=False, key=lambda l: l[1])\n        result = result[-beam_index:]\n        \n        \n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        prd_id = pred_list[-1] \n        if(prd_id!=3):\n            dec_input = tf.expand_dims([prd_id], 0)  \n        else:\n            break\n    \n    \n    result2 = result[-1][0]\n    \n    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n    final_caption = []\n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n            \n        else:\n            break\n\n    attention_plot = attention_plot[:len(result), :]\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_attmap(caption, weights, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image))\n    \n    len_cap = len(caption)\n    for cap in range(len_cap):\n        weights_img = np.reshape(weights[cap], (8,8))\n        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n        \n        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n        ax.set_title(caption[cap], fontsize=15)\n        \n        img=ax.imshow(temp_img)\n        \n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path = \"..working/checkpoints/train/ckpt-5\"\n   \nckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer = optimizer)\nckpt.restore(checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rid = np.random.randint(0, len(img_test))\ntest_image = img_test[rid]\n#test_image = './dataset/Flickr8k/Flicker8k_Dataset/413231421_43833a11f5.jpg'\n#real_caption = '<start> black dog is digging in the snow <end>'\n\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\nresult, attention_plot,pred_test = evaluate(test_image)\n\n\nreal_caption=filt_text(real_caption)      \n\n\npred_caption=' '.join(result).rsplit(' ', 1)[0]\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = pred_caption.split()\n\nscore = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\nprint(f\"BELU score: {score*100}\")\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', pred_caption)\nplot_attmap(result, attention_plot, test_image)\n\n\nImage.open(test_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"captions=beam_evaluate(test_image)\nprint(captions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}